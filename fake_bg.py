# Get card with largest similarity

# Train the current transform if it's right
# Otherwise, discard it
# When the reward dies after multiple successes, switch rules

# shit, how do I get an array of integrators and how do I gate them?
# can I just do the inhibition thing?
# and then add all the outputs at the end?

# Do I need mechanism to make sure that's more gradual the first time a rule change happens?

# I really feel like I need a mechanism to learn from failures, but I can't figure out how. I guess I have to run the model and find out the hard way.